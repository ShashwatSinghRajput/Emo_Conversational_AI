{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "841b8dc7-d8a2-421e-aefb-032c0e96e7a6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting faster-whisper\n",
      "  Using cached faster_whisper-1.0.1-py3-none-any.whl.metadata (14 kB)\n",
      "Collecting av==11.* (from faster-whisper)\n",
      "  Using cached av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.5 kB)\n",
      "Collecting ctranslate2<5,>=4.0 (from faster-whisper)\n",
      "  Using cached ctranslate2-4.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (10 kB)\n",
      "Collecting huggingface-hub>=0.13 (from faster-whisper)\n",
      "  Using cached huggingface_hub-0.22.2-py3-none-any.whl.metadata (12 kB)\n",
      "Collecting tokenizers<0.16,>=0.13 (from faster-whisper)\n",
      "  Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting onnxruntime<2,>=1.14 (from faster-whisper)\n",
      "  Using cached onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.4 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from ctranslate2<5,>=4.0->faster-whisper) (68.2.2)\n",
      "Requirement already satisfied: numpy in /opt/conda/lib/python3.10/site-packages (from ctranslate2<5,>=4.0->faster-whisper) (1.24.4)\n",
      "Requirement already satisfied: pyyaml<7,>=5.3 in /opt/conda/lib/python3.10/site-packages (from ctranslate2<5,>=4.0->faster-whisper) (6.0.1)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster-whisper) (3.9.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster-whisper) (2023.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster-whisper) (23.2)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster-whisper) (2.31.0)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster-whisper) (4.66.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub>=0.13->faster-whisper) (4.8.0)\n",
      "Collecting coloredlogs (from onnxruntime<2,>=1.14->faster-whisper)\n",
      "  Using cached coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: flatbuffers in /opt/conda/lib/python3.10/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (23.5.26)\n",
      "Requirement already satisfied: protobuf in /opt/conda/lib/python3.10/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (4.23.4)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from onnxruntime<2,>=1.14->faster-whisper) (1.12)\n",
      "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<2,>=1.14->faster-whisper)\n",
      "  Using cached humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.3.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (3.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2.0.7)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->huggingface-hub>=0.13->faster-whisper) (2023.7.22)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->onnxruntime<2,>=1.14->faster-whisper) (1.3.0)\n",
      "Using cached faster_whisper-1.0.1-py3-none-any.whl (1.5 MB)\n",
      "Using cached av-11.0.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (32.9 MB)\n",
      "Using cached ctranslate2-4.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.7 MB)\n",
      "Using cached huggingface_hub-0.22.2-py3-none-any.whl (388 kB)\n",
      "Using cached onnxruntime-1.17.3-cp310-cp310-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.8 MB)\n",
      "Using cached tokenizers-0.15.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Using cached coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
      "Using cached humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
      "Installing collected packages: humanfriendly, ctranslate2, av, huggingface-hub, coloredlogs, tokenizers, onnxruntime, faster-whisper\n",
      "Successfully installed av-11.0.0 coloredlogs-15.0.1 ctranslate2-4.1.0 faster-whisper-1.0.1 huggingface-hub-0.22.2 humanfriendly-10.0 onnxruntime-1.17.3 tokenizers-0.15.2\n",
      "Collecting jiwer\n",
      "  Using cached jiwer-3.0.3-py3-none-any.whl.metadata (2.6 kB)\n",
      "Requirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\n",
      "Collecting rapidfuzz<4,>=3 (from jiwer)\n",
      "  Using cached rapidfuzz-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Using cached jiwer-3.0.3-py3-none-any.whl (21 kB)\n",
      "Using cached rapidfuzz-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
      "Installing collected packages: rapidfuzz, jiwer\n",
      "Successfully installed jiwer-3.0.3 rapidfuzz-3.8.1\n",
      "Collecting pydub\n",
      "  Using cached pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Using cached pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
      "Installing collected packages: pydub\n",
      "Successfully installed pydub-0.25.1\n"
     ]
    }
   ],
   "source": [
    "!pip install faster-whisper\n",
    "!pip install jiwer\n",
    "!pip install pydub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e08f2966-cd2d-44a5-91a9-bc2f45301ff9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated LD_LIBRARY_PATH: /opt/conda/lib/python3.10/site-packages/nvidia/cublas/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# The directory where libcublas.so.12 is located\n",
    "cublas_directory = '/opt/conda/lib/python3.10/site-packages/nvidia/cublas/lib'\n",
    "\n",
    "# Get the current LD_LIBRARY_PATH\n",
    "current_ld_library_path = os.environ.get('LD_LIBRARY_PATH', '')\n",
    "\n",
    "# If the cublas directory is not already in LD_LIBRARY_PATH, add it\n",
    "if cublas_directory not in current_ld_library_path:\n",
    "    os.environ['LD_LIBRARY_PATH'] = cublas_directory + ':' + current_ld_library_path\n",
    "\n",
    "# Print the updated LD_LIBRARY_PATH to verify the change\n",
    "print(\"Updated LD_LIBRARY_PATH:\", os.environ['LD_LIBRARY_PATH'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c48c913-a5d6-44b9-bd80-5fdf56cb4ca6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Library loaded successfully!\n"
     ]
    }
   ],
   "source": [
    "import ctypes\n",
    "import os\n",
    "\n",
    "# Path to the libcublas library\n",
    "libcublas_path = '/opt/conda/lib/python3.10/site-packages/nvidia/cublas/lib/libcublas.so.12'\n",
    "\n",
    "# Try loading the library directly\n",
    "try:\n",
    "    ctypes.CDLL(libcublas_path)\n",
    "    print(\"Library loaded successfully!\")\n",
    "except OSError as e:\n",
    "    print(\"Failed to load library:\", str(e))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b8622e-0943-4f71-9443-c434c012238c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LD_LIBRARY_PATH: /opt/conda/lib/python3.10/site-packages/nvidia/cublas/lib:/usr/local/nvidia/lib:/usr/local/nvidia/lib64\n",
      "PATH: /opt/conda/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Print environment variables related to library paths\n",
    "print(\"LD_LIBRARY_PATH:\", os.environ.get('LD_LIBRARY_PATH'))\n",
    "print(\"PATH:\", os.environ.get('PATH'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8c77a62e-39b9-4bc5-ae71-2f49b05732e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import time\n",
    "from faster_whisper import WhisperModel\n",
    "import faster_whisper\n",
    "from jiwer import wer, cer\n",
    "model_size = \"large-v3\"\n",
    "\n",
    "# Initialize WhisperModel for fast processing on appropriate hardware\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "# model = faster_whisper.WhisperModel(\"large-v3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ced793a6-9873-4f96-ac63-d124ffa6c79a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected language 'en' with probability 0.985840\n",
      "Transcribed Text:  The WISPR models are trained for speech recognition and translation tasks.  Capable of transcribing speech audio into the text in the language it is spoken,  which is also known as automatic speech recognition,  as well as translated into speech, especially in English, which is speech translation.  Researchers at OpenAI developed the model to study the robustness of speech processing systems  trained under large-scale weak supervision.  There are nine models of different sizes and capabilities summarized in the following table.  According to the size, the tiny model has 39 million parameters and it is English only.  Base model has a size of 3.5 million parameters.  It has 74 million parameters and it is also English only model.  Then there is a small model, which has around 244 million parameters and it is also English only.  Then there is a medium model.  The medium model has 796 million parameters and it is English only model as well.  Then there is a large model as well, which has 100, sorry, it has 1000,  550 million parameters and it is a multilingual model.  Thank you.\n",
      "WER: 0.06395348837209303\n",
      "CER: 0.06492248062015504\n",
      "Latency: 0.6522796154022217 seconds\n"
     ]
    }
   ],
   "source": [
    "# import time\n",
    "# from faster_whisper import WhisperModel\n",
    "# from jiwer import wer, cer\n",
    "\n",
    "# model_size = \"large-v3\"\n",
    "\n",
    "# Initialize WhisperModel\n",
    "# model = WhisperModel(model_size, device=\"cpu\", compute_type=\"int8\")\n",
    "\n",
    "# Run on GPU with FP16\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"float16\")\n",
    "\n",
    "# or run on GPU with INT8\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "\n",
    "# Define the audio file path\n",
    "audio_file = \"Whisper.mp3\"\n",
    "\n",
    "# Reference sentence\n",
    "reference_text = \"The Whisper models are trained for speech recognition and translation tasks. Capable of transcribing speech audio into the text in the language it is spoken, which is also known as automatic speech recognition, as well as translated into speech, especially in English, which is speech translation. Researchers at OpenAI developed the model to study the robustness of speech processing systems trained under large-scale weak supervision. There are nine models of different sizes and capabilities summarized in the following table. According to the size, the tiny model has 39 million parameters and it is English only. Base model has 74 million parameters and it is also English only model. Then there is a small model, which has around 244 million parameters and it is also English only. Then there is a medium model. The medium model has 796 million parameters and it is English only model as well. Then there is a large model as well, which has 100, sorry, it has 1550 million parameters and it is a multilingual model. Thank you. \"\n",
    "\n",
    "# Transcribe audio and measure latency\n",
    "start_time = time.time()\n",
    "segments, info = model.transcribe(audio_file, beam_size=5)\n",
    "end_time = time.time()\n",
    "latency = end_time - start_time\n",
    "\n",
    "print(\"Detected language '%s' with probability %f\" % (info.language, info.language_probability))\n",
    "\n",
    "# Process segments and compute WER and CER\n",
    "transcribed_text = \" \".join(segment.text for segment in segments)\n",
    "wer_result = wer(reference_text, transcribed_text)\n",
    "cer_result = cer(reference_text, transcribed_text)\n",
    "\n",
    "# Print results\n",
    "print(\"Transcribed Text:\", transcribed_text)\n",
    "print(\"WER:\", wer_result)\n",
    "print(\"CER:\", cer_result)\n",
    "print(\"Latency:\", latency, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "14ce506b-1c54-494a-9ff1-2272f948f52b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Latency for segment: 0.4292299747467041 seconds\n",
      " The whisper models are trained for speech recognition and translation.\n",
      "Latency for segment: 0.46895265579223633 seconds\n",
      " Capable of transcribing speech audio into the text\n",
      "Latency for segment: 0.6677262783050537 seconds\n",
      " In the language, it is spoken, which is also known as automatic speech.\n",
      "Latency for segment: 0.366931676864624 seconds\n",
      " recognition as well as translated into speech\n",
      "Latency for segment: 0.5230717658996582 seconds\n",
      " especially in English, which is speech translation.\n",
      "Latency for segment: 0.380403995513916 seconds\n",
      " Researchers at OpenAI developed the model to study the robust\n",
      "Latency for segment: 0.5705561637878418 seconds\n",
      " of speech processing systems trained under large-scale\n",
      "Latency for segment: 0.48041868209838867 seconds\n",
      " supervision. There are nine models of different sizes and capabilities\n",
      "Latency for segment: 0.4397740364074707 seconds\n",
      " summarized in the following table.\n",
      "Latency for segment: 0.3830268383026123 seconds\n",
      " According to the size, the tiny model has 39 million parameters.\n",
      "Latency for segment: 0.5059428215026855 seconds\n",
      " and it is English only base model has\n",
      "Latency for segment: 0.44492053985595703 seconds\n",
      " 74 million parameters and it is also English only model.\n",
      "Latency for segment: 0.31236767768859863 seconds\n",
      " there is a small model\n",
      " which has around\n",
      " 244 million paragraphs\n",
      "Latency for segment: 0.3321542739868164 seconds\n",
      " parameters and it is also English only. Then there is a medium\n",
      "Latency for segment: 0.40276122093200684 seconds\n",
      " model. The medium model has 796 million parameters\n",
      "Latency for segment: 0.3438534736633301 seconds\n",
      " it is English only model as well. Then there is a large model\n",
      "Latency for segment: 0.44963884353637695 seconds\n",
      " as well\n",
      " which has\n",
      " 100\n",
      " sorry\n",
      " it has\n",
      " 1500\n",
      "Latency for segment: 0.42606019973754883 seconds\n",
      " 550 million parameters and it is a multilingual model.\n",
      "Latency for segment: 0.3997340202331543 seconds\n",
      " Thank you.\n",
      "\n",
      "Overall WER: 0.12790697674418605\n",
      "Overall CER: 0.06298449612403101\n",
      "Total transcription latency: 8.32752513885498 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from pydub import AudioSegment\n",
    "from faster_whisper import WhisperModel\n",
    "from jiwer import wer, cer\n",
    "\n",
    "# model_size = \"large-v3\"\n",
    "\n",
    "# # Initialize WhisperModel for fast processing on appropriate hardware\n",
    "# model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "\n",
    "# Define the audio file and split duration\n",
    "audio_file = \"Whisper.wav\"\n",
    "split_duration = 5000  # 5 seconds per chunk, in milliseconds\n",
    "\n",
    "# Load the audio file correctly\n",
    "audio = AudioSegment.from_wav(audio_file)\n",
    "\n",
    "# Reference text for WER and CER calculation\n",
    "reference_text = \"The Whisper models are trained for speech recognition and translation tasks. Capable of transcribing speech audio into the text in the language it is spoken, which is also known as automatic speech recognition, as well as translated into speech, especially in English, which is speech translation. Researchers at OpenAI developed the model to study the robustness of speech processing systems trained under large-scale weak supervision. There are nine models of different sizes and capabilities summarized in the following table. According to the size, the tiny model has 39 million parameters and it is English only. Base model has 74 million parameters and it is also English only model. Then there is a small model, which has around 244 million parameters and it is also English only. Then there is a medium model. The medium model has 796 million parameters and it is English only model as well. Then there is a large model as well, which has 100, sorry, it has 1550 million parameters and it is a multilingual model. Thank you.\"\n",
    "\n",
    "# Function to process and transcribe each audio segment\n",
    "def transcribe_segment(model, segment):\n",
    "    # Export segment to a temporary audio file - consider using BytesIO instead for in-memory operation\n",
    "    segment.export(\"temp_segment.wav\", format=\"wav\")\n",
    "    start_time = time.time()  # Start timing\n",
    "    segments, info = model.transcribe(\"temp_segment.wav\", beam_size=5)\n",
    "    end_time = time.time()  # End timing\n",
    "    latency = end_time - start_time\n",
    "    return segments, latency\n",
    "\n",
    "# Prepare to collect results\n",
    "transcribed_text = \"\"\n",
    "total_latency = 0  # Initialize total latency\n",
    "\n",
    "# Split and transcribe the audio in chunks\n",
    "for i in range(0, len(audio), split_duration):\n",
    "    segment = audio[i:i + split_duration]\n",
    "    transcription_segments, segment_latency = transcribe_segment(model, segment)\n",
    "    total_latency += segment_latency  # Accumulate latency for each segment\n",
    "    print(f\"Latency for segment: {segment_latency} seconds\")\n",
    "    for transcription in transcription_segments:\n",
    "        transcribed_text += transcription.text + \" \"\n",
    "        print(transcription.text)  # Output the transcription of each segment\n",
    "\n",
    "# Calculate WER and CER\n",
    "computed_wer = wer(reference_text, transcribed_text)\n",
    "computed_cer = cer(reference_text, transcribed_text)\n",
    "\n",
    "print(\"\\nOverall WER:\", computed_wer)\n",
    "print(\"Overall CER:\", computed_cer)\n",
    "print(\"Total transcription latency:\", total_latency, \"seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "62f7b2f1-e2d6-4bab-8da1-c3d70615ff05",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Transcriptions: 100%|██████████| 100/100 [01:32<00:00,  1.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing complete. Results saved to 'transcription_results.csv'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torchaudio\n",
    "from torchaudio.datasets import LIBRISPEECH\n",
    "from faster_whisper import WhisperModel\n",
    "from jiwer import wer, cer\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import csv\n",
    "\n",
    "# Initialize the Whisper model\n",
    "model_size = \"large-v3\"\n",
    "model = WhisperModel(model_size, device=\"cuda\", compute_type=\"int8_float16\")\n",
    "\n",
    "# Load the LibriSpeech test dataset\n",
    "librispeech_test = LIBRISPEECH(root=\"./data\", url=\"test-clean\", download=True)\n",
    "\n",
    "def transcribe_audio(model, waveform, sample_rate):\n",
    "    torchaudio.save(\"temp_audio.wav\", waveform, sample_rate)\n",
    "    start_time = time.time()\n",
    "    segments, info = model.transcribe(\"temp_audio.wav\", beam_size=5)\n",
    "    end_time = time.time()\n",
    "    latency = end_time - start_time\n",
    "    transcription = ' '.join([segment.text for segment in segments])\n",
    "    return transcription, latency\n",
    "\n",
    "# Evaluate on a set number of samples from the dataset\n",
    "num_samples = 100\n",
    "results = []\n",
    "\n",
    "# Setting up CSV file to record the results\n",
    "with open('transcription_results.csv', mode='w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow(['Sample ID', 'Original Text', 'Transcribed Text', 'WER', 'CER', 'Latency (s)'])\n",
    "\n",
    "    progress_bar = tqdm(total=num_samples, desc=\"Processing Transcriptions\")\n",
    "    for i, (waveform, sample_rate, utterance, speaker_id, chapter_id, utterance_id) in enumerate(librispeech_test):\n",
    "        if i >= num_samples:\n",
    "            break\n",
    "        transcription, latency = transcribe_audio(model, waveform, sample_rate)\n",
    "        current_wer = wer(utterance.lower(), transcription.lower())\n",
    "        current_cer = cer(utterance.lower(), transcription.lower())\n",
    "        # Writing results to CSV\n",
    "        writer.writerow([i+1, utterance, transcription, f\"{current_wer:.2f}\", f\"{current_cer:.2f}\", f\"{latency:.2f}\"])\n",
    "        progress_bar.update(1)\n",
    "    progress_bar.close()\n",
    "\n",
    "print(\"Processing complete. Results saved to 'transcription_results.csv'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be70469f-28ff-4ae7-9d86-b1c10101956d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
